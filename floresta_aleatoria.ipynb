{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNlifPetTTbo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1y1PEKG29Uv"
      },
      "outputs": [],
      "source": [
        "include_metrics = ['cpu_app', 'memory_app', 'cpu_db', 'memory_db', 'rows_fetched', 'rows_returned', 'db_commits', 'response_time']\n",
        "exclude_from_calls = ['timestamp_begin', 'timestamp_end', 'total_calls'] + include_metrics\n",
        "test_sample = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF3hRIskVECm"
      },
      "source": [
        "Definindo o caminho para os arquivos CSV contendo os resultados dos testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZrepwpAT7ts"
      },
      "outputs": [],
      "source": [
        "DATA_CSV_PATH_BASE = '/dados/data_versao_boa_1.csv'\n",
        "DATA_CSV_PATH_TARGET = '/dados/data_versao_boa_2.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUo2pBMPVTlT"
      },
      "source": [
        "Definindo a função que irá transformar os dados do arquivo CSV em arrays para utilizar as funções so scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDxLJBvhCGth"
      },
      "outputs": [],
      "source": [
        "def recupera_outliers(df, target='cpu'):\n",
        "  outliers_indexes = []\n",
        "\n",
        "  q1 = df[target].quantile(0.25)\n",
        "  q3 = df[target].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  maximum = q3 + (1.5 * iqr)\n",
        "  minimum = q1 - (1.5 * iqr)\n",
        "\n",
        "  outlier_samples = df[(df[target] < minimum) | (df[target] > maximum)]\n",
        "  outliers_indexes.extend(outlier_samples.index.tolist())\n",
        "\n",
        "  outliers_indexes = list(set(outliers_indexes))\n",
        "\n",
        "  return outliers_indexes\n",
        "\n",
        "def populate_arrays(data_csv):\n",
        "\n",
        "    metrics = data_csv[include_metrics].values.tolist()\n",
        "    calls = data_csv.drop(exclude_from_calls, axis=1).values.tolist()\n",
        "\n",
        "    return [calls, metrics]\n",
        "\n",
        "def handle_test(dados_csv):\n",
        "    result = populate_arrays(dados_csv)\n",
        "    requisicoes = result[0]\n",
        "    metricas_conjuntas = result[1]\n",
        "\n",
        "    florestas_aleatorias = []\n",
        "    metricas= []\n",
        "\n",
        "    for index in range(len(include_metrics)):\n",
        "      metrica = []\n",
        "      for linha in metricas_conjuntas:\n",
        "        metrica.append(linha[index])\n",
        "\n",
        "      metricas.append(metrica)\n",
        "      floresta_aleatoria = RandomForestRegressor(random_state=0)\n",
        "      floresta_aleatoria = floresta_aleatoria.fit(requisicoes, metrica)\n",
        "      florestas_aleatorias.append(floresta_aleatoria)\n",
        "\n",
        "    return [florestas_aleatorias, requisicoes, metricas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzdRDn3B0IUd"
      },
      "outputs": [],
      "source": [
        "def get_metrics(random_forest_base, calls_alvo):\n",
        "  metrics_predict = []\n",
        "\n",
        "  for index in range(len(include_metrics)):\n",
        "    metrics_predict.append(random_forest_base[index].predict(calls_alvo))\n",
        "\n",
        "  return metrics_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEBhINTBrN5a"
      },
      "outputs": [],
      "source": [
        "def calculate_mape(random_forest_base, metrics_alvo, calls_alvo):\n",
        "  mapes = []\n",
        "\n",
        "  metrics_predicted = get_metrics(random_forest_base, calls_alvo)\n",
        "\n",
        "  for index in range(len(include_metrics)):\n",
        "    mapes.append(mean_absolute_percentage_error(metrics_predicted[index], metrics_alvo[index]))\n",
        "\n",
        "  return mapes\n",
        "\n",
        "def print_mape(index, mapes):\n",
        "  for metric in range(len(include_metrics)):\n",
        "    print(f\"[MAPE TESTE {index}] - {include_metrics[metric]}: {round(mapes[metric]*100,3)}%\")\n",
        "  print(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8N4w8EZkPD-"
      },
      "outputs": [],
      "source": [
        "def calculate_cliff_delta(metrics_base, metrics_alvo):\n",
        "    threshold = {'pequena': 0.147, 'media': 0.33, 'grande': 0.474}\n",
        "\n",
        "    size = len(metrics_base)\n",
        "    sum = 0\n",
        "\n",
        "    for index_base in range(len(metrics_base)):\n",
        "      for index_alvo in range(len(metrics_base)):\n",
        "        value = 0\n",
        "\n",
        "        if metrics_base[index_base] < metrics_alvo[index_alvo]:\n",
        "          value = -1\n",
        "\n",
        "        elif metrics_base[index_base] > metrics_alvo[index_alvo]:\n",
        "          value = 1\n",
        "\n",
        "        sum += value\n",
        "\n",
        "    delta = sum / (size*size)\n",
        "    size = eval_cliff_delta(delta, threshold)\n",
        "    return delta, size\n",
        "\n",
        "def eval_cliff_delta(delta: float, threshold: dict) -> str:\n",
        "    delta = abs(delta)\n",
        "    if delta < threshold['pequena']:\n",
        "        return 'trivial'\n",
        "    if threshold['pequena'] <= delta < threshold['media']:\n",
        "        return 'pequena'\n",
        "    if threshold['media'] <= delta < threshold['grande']:\n",
        "        return 'media'\n",
        "    if delta >= threshold['grande']:\n",
        "        return 'grande'\n",
        "\n",
        "def print_cliff_delta(random_forest_base, metrics_target, calls_alvo):\n",
        "  good = 'POSITIVA'\n",
        "  bad = 'NEGATIVA'\n",
        "\n",
        "  metrics_predicted = get_metrics(random_forest_base, calls_alvo)\n",
        "\n",
        "  for index in range(len(include_metrics)):\n",
        "    cliff = calculate_cliff_delta(metrics_predicted[index], metrics_target[index])\n",
        "    result = cliff[0] > 0 and good or bad\n",
        "\n",
        "    if index == 2:\n",
        "      print(f\"metric: {include_metrics[index].upper()} \\t\\t\\t| cliff delta: {round(cliff[0],3)} \\t\\t| DIFERENÇA: {result} {cliff[1].upper()}\\n\")\n",
        "    else:\n",
        "      print(f\"metric: {include_metrics[index].upper()} \\t\\t| cliff delta: {round(cliff[0],3)} \\t\\t| DIFERENÇA: {result} {cliff[1].upper()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds8yLg1ZIaQF"
      },
      "outputs": [],
      "source": [
        "def print_accuracy(rfs, metrics, calls, test_index):\n",
        "  for metric in range(len(include_metrics)):\n",
        "    accuracy = calculate_accuracy(rfs, metrics, calls, metric)\n",
        "    print(f\"Test {test_index} - R² {include_metrics[metric]}: {accuracy}\")\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "def r_quadrado(metrica_real, metrica_prevista):\n",
        "  soma_real = 0\n",
        "  soma_dif_predict = 0\n",
        "  soma_dif_media = 0\n",
        "\n",
        "  for indice in range(len(metrica_real)):\n",
        "    soma_real += metrica_real[indice]\n",
        "    aux = (metrica_real[indice] - metrica_prevista[indice])\n",
        "    soma_dif_predict += (aux * aux)\n",
        "\n",
        "  media = soma_real/len(metrica_real)\n",
        "\n",
        "  for indice in range(len(metrica_real)):\n",
        "    aux = (metrica_real[indice] - media)\n",
        "    soma_dif_media += (aux * aux)\n",
        "\n",
        "  return 1 - (soma_dif_predict/soma_dif_media)\n",
        "\n",
        "\n",
        "def calculate_accuracy(rfs, metrics, calls, metric_index):\n",
        "\n",
        "  metric_predict = rfs[metric_index].predict(calls)\n",
        "  r2 = r_quadrado(metrics[metric_index], metric_predict)\n",
        "  r2_ajustado = 1-(1-r2)*(len(metrics)-1)/(len(metrics)-len(include_metrics)-1)\n",
        "  return r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSH_WF1NaqRA"
      },
      "source": [
        "Leitura de dados e remoção de outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJHS0omyUdki"
      },
      "outputs": [],
      "source": [
        "data_csv_target = pd.read_csv(DATA_CSV_PATH_TARGET)\n",
        "data_csv_base = pd.read_csv(DATA_CSV_PATH_BASE).head(len(data_csv_target))\n",
        "\n",
        "outliers_indexes = list(set([*recupera_outliers(data_csv_base, 'total_calls'), *recupera_outliers(data_csv_target, 'total_calls')]))\n",
        "\n",
        "print(f'Indices dos outliers encontrados: {outliers_indexes}')\n",
        "\n",
        "data_csv_base.drop(outliers_indexes, inplace=True)\n",
        "data_csv_base.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data_csv_target.drop(outliers_indexes, inplace=True)\n",
        "data_csv_target.reset_index(drop=True, inplace=True)\n",
        "\n",
        "result_base = handle_test(data_csv_base)\n",
        "rf_base = result_base[0]\n",
        "calls_base = result_base[1]\n",
        "metrics_base = result_base[2]\n",
        "\n",
        "result_target = handle_test(data_csv_target)\n",
        "rf_target = result_target[0]\n",
        "calls_target = result_target[1]\n",
        "metrics_target = result_target[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo de R² ajustado"
      ],
      "metadata": {
        "id": "n9HuL2yIgvwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjTprk1zWkxh"
      },
      "outputs": [],
      "source": [
        "print_accuracy(rf_base, metrics_target, calls_target, \"BASE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo MAPE comparando:\n",
        "\n",
        "1.   versão base x versão base\n",
        "2.   versão base x versão base normalizada\n",
        "\n"
      ],
      "metadata": {
        "id": "w9GxdaQqgdgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG4DLA210hMw"
      },
      "outputs": [],
      "source": [
        "print_mape(\"BASE\", calculate_mape(rf_base, metrics_base, calls_base))\n",
        "print_mape(\"TARGET\", calculate_mape(rf_base, metrics_target, calls_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6kngZSGpcx8"
      },
      "source": [
        "Interpretação do Cliff Delta:\n",
        "\n",
        "A métrica prevista pelo Random Forest representa como o sistema se comportaria recebendo aquela quantidade de chamada no ambiente de testes configurado no teste 1. Portanto, se o valor da métrica prevista for maior que o valor da métrica real (capturada no ambiente do teste 2), significa que houve uma melhora no sistema quando comparamos a versão 1 com a versão 2.\n",
        "\n",
        "Valor **positivo** representa que o **primeiro** teste possui valores **maiores**\n",
        "\n",
        "Valor **negativo** representa que o **segundo** teste possui valores **maiores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p--JVT6S1kBA"
      },
      "outputs": [],
      "source": [
        "print_cliff_delta(rf_base, metrics_target, calls_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráficos ilustrando processo de normalização de escala utilizando Floresta Aleatória."
      ],
      "metadata": {
        "id": "Dlt3d3-HgM-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z99aOTXzW8bU"
      },
      "outputs": [],
      "source": [
        "traduz_metricas = {\n",
        "    'cpu_app': 'Uso de CPU % - Aplicação',\n",
        "    'memory_app': 'Uso de Memória RAM % - Aplicação',\n",
        "    'cpu_db': 'Uso de CPU % - Banco de dados',\n",
        "    'memory_db': 'Uso de Memória RAM % - Banco de dados',\n",
        "    'rows_fetched': 'Linhas buscadas',\n",
        "    'rows_returned': 'Linhas retornadas',\n",
        "    'db_commits': 'Transações concluídas',\n",
        "    'response_time': 'Tempo de resposta'\n",
        "}\n",
        "\n",
        "def plot_grafico(df_base, df_target, metric, ax, label_base, label_target):\n",
        "  ax.plot(df_base, 'b-', label=label_base)\n",
        "  ax.plot(df_target, 'r--', label=label_target)\n",
        "  ax.legend(loc='lower right')\n",
        "  ax.set_xlabel('Período de tempo (minuto)', fontsize = 14)\n",
        "  ax.set_ylabel(f'{traduz_metricas[metric]}', fontsize = 14)\n",
        "\n",
        "fig, ax = plt.subplots(len(include_metrics), 3, figsize=(15,35))\n",
        "\n",
        "for index, metric in enumerate(include_metrics):\n",
        "  metric_true_base = metrics_base[index]\n",
        "  metric_true_target = metrics_target[index]\n",
        "  metric_predict_target = rf_base[index].predict(calls_target)\n",
        "\n",
        "  plot_grafico(metric_true_base, metric_true_target, metric, ax[index, 0], 'Teste base', 'Teste alvo')\n",
        "  plot_grafico(metric_true_base, metric_predict_target, metric, ax[index, 1], 'Teste base', 'Teste base normalizado')\n",
        "  plot_grafico(metric_predict_target, metric_true_target, metric, ax[index, 2], 'Teste base normalizado', 'Teste alvo')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}